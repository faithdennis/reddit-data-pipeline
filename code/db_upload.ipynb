{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9c9ee8-8844-45a5-84fa-c4f20877d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f80d3-d54b-4a1f-8b69-c3530cf76b31",
   "metadata": {},
   "source": [
    "#### Check if data files have been extracted\n",
    "\n",
    "In theory, this notebook should be run after `EDA.ipynb` where we perform our intial data exploratory.\n",
    "In that notebook, we unzip the compressed data file into two `.ndjson` for submission and comment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b5f3d7-a6da-437a-ab5d-44ebea19195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_data_path = os.path.join(\"..\", \"data\", \"extracted_data\", \"comments_data.ndjson\")\n",
    "submissions_data_path = os.path.join(\"..\", \"data\", \"extracted_data\", \"submissions_data.ndjson\")\n",
    "\n",
    "assert os.path.isfile(comments_data_path) is True\n",
    "assert os.path.isfile(submissions_data_path) is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9451975d-e75f-404a-a0d4-d9eb9b19e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_db(db_engine, data_file: str, table_name: str, selected_columns: list[str]):\n",
    "    \"\"\"\n",
    "    Upload to database\n",
    "    \"\"\"\n",
    "\n",
    "    assert os.path.isfile(data_file) is True\n",
    "    assert data_file.endswith('.ndjson') is True\n",
    "\n",
    "    with open(data_file, 'r') as f:\n",
    "        done = False\n",
    "        it = 1\n",
    "        while not done:\n",
    "            jsons = []\n",
    "            # 1000 lines at a time\n",
    "            for i in range(1000):\n",
    "                line = f.readline().strip()\n",
    "\n",
    "                # end of line is reached\n",
    "                if not line:\n",
    "                    done = True\n",
    "                    break\n",
    "\n",
    "                json_line = json.loads(line)\n",
    "                jsons.append(json_line)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            df = pd.DataFrame(jsons)\n",
    "            # only select relevant columns\n",
    "            for col in selected_columns:\n",
    "                # NOTE: some json entries may not contain the selected column,  \n",
    "                # we will that as NA\n",
    "                if col not in df.columns:\n",
    "                    df[col] = pd.NA\n",
    "\n",
    "            df = df[selected_columns]\n",
    "            df.to_sql(table_name, db_engine, if_exists='append', index=False)\n",
    "            print(f'iteration #{it}: data written to {table_name} successfully.')\n",
    "            it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694f3d48-7ac8-40a5-a6fa-5d51e978fed7",
   "metadata": {},
   "source": [
    "#### Create database engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "618ce8f0-b1da-47b1-8f8c-5e4ccba4e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = '../.env'\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "def get_db_url(db_name):\n",
    "    db_str = 'postgresql+psycopg2://{}:{}@{}:{}/{}'\n",
    "    return db_str.format(\n",
    "        os.getenv('DB_USERNAME'),\n",
    "        os.getenv('DB_PASSWORD'),\n",
    "        os.getenv('DB_HOST'),\n",
    "        os.getenv('DB_PORT'),\n",
    "        db_name\n",
    "    )\n",
    "\n",
    "db_url = get_db_url('reddit')\n",
    "db_engine = create_engine(db_url)\n",
    "\n",
    "# TODO: ensure db_engine is successfully created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b34a83-7e35-49d6-a4fa-1f01a83a159b",
   "metadata": {},
   "source": [
    "#### Upload data to database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1b68a-1cc7-4082-83ab-da9649056c08",
   "metadata": {},
   "source": [
    "Comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ed899-1fd1-48a6-9c99-9908e1fdb03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define relevant comment columns\n",
    "# this is the result of the EDA we did on the Reddit dataset\n",
    "relevant_comment_columns = [\n",
    "    \"id\", \"archived\", \"author\", \"author_created_utc\", \n",
    "    \"author_fullname\", \"body\", \"controversiality\", \n",
    "    \"created_utc\", \"downs\", \"locked\", \n",
    "    \"name\", \"num_reports\", \"parent_id\", \"permalink\", \n",
    "    \"retrieved_on\", \"score\", \"subreddit\", \n",
    "    \"subreddit_id\", \"subreddit_name_prefixed\", \n",
    "    \"subreddit_type\", \"total_awards_received\", \n",
    "    \"updated_on\", \"ups\"\n",
    "]\n",
    "\n",
    "upload_to_db(db_engine, comments_data_path, 'comments', relevant_comment_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e872d-d8e3-4174-a9c0-4c66548e7421",
   "metadata": {},
   "source": [
    "Submissions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaab3eb-f490-4c96-a866-ce2ee9ffe4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define relevant submission columns\n",
    "# this is the result of the EDA we did on the Reddit dataset\n",
    "relevant_submission_columns = [\n",
    "    \"id\", \n",
    "    \"archived\", \n",
    "    \"author\", \n",
    "    \"author_created_utc\", \n",
    "    \"author_fullname\", \n",
    "    \"created_utc\", \n",
    "    \"downs\", \n",
    "    \"is_self\", \n",
    "    \"locked\", \n",
    "    \"name\", \n",
    "    \"num_comments\", \n",
    "    \"num_crossposts\", \n",
    "    \"num_reports\", \n",
    "    \"permalink\", \n",
    "    \"score\", \n",
    "    \"selftext\", \n",
    "    \"spoiler\", \n",
    "    \"subreddit\", \n",
    "    \"subreddit_id\", \n",
    "    \"subreddit_name_prefixed\", \n",
    "    \"subreddit_subscribers\", \n",
    "    \"subreddit_type\", \n",
    "    \"title\", \n",
    "    \"total_awards_received\", \n",
    "    \"ups\", \n",
    "    \"upvote_ratio\", \n",
    "    \"url\"\n",
    "]\n",
    "\n",
    "upload_to_db(db_engine, submissions_data_path, 'submissions', relevant_submission_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c141d3-a53f-4e74-8315-72b21a69c5c5",
   "metadata": {},
   "source": [
    "### Normalize Reddit dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84042cf-2d70-47f9-806b-a84f5dc03f8f",
   "metadata": {},
   "source": [
    "#### `Subreddit`\n",
    "From inspection of the two data tables `submissions` and `comments` and their attributes, it appears that the dataset is fairly denormalized.\n",
    "For instance, entries in both `submissions` and `comments` have attributes `subreddit`, `subreddit_id`, `subreddit_name_prefixed`, `subreddit_type`. Entries in `submissions` also contain `subreddit_subscribers`, which is the number of subscribers to the subreddit. This might be useful to track the number of subsribers to a subreddit over time.\n",
    "\n",
    "\n",
    "From the observations, `subreddit` can be regarded as a separate entity set which allows the retrieval of info about subreddits easier. This makes sense, as in our dataset, there are only posts and comments from 13 different subreddits.\n",
    "\n",
    "In this code below, we will further explore information on subreddits stored in both `submissions` and `comments` and create a seperate table for `subreddit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8584ee8b-24b5-44d3-aa30-0ae3b3909997",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_columns = [\n",
    "    \"subreddit\", \n",
    "    \"subreddit_id\", \n",
    "    \"subreddit_name_prefixed\", \n",
    "    \"subreddit_subscribers\", \n",
    "    \"subreddit_type\" \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119c0c8-23ec-49d2-b19e-f51ff3f00ffc",
   "metadata": {},
   "source": [
    "Connect to database using `psycopg2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75003735-a5e6-4f56-9cbd-3b661dd5c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    dbname='reddit',\n",
    "    user=os.getenv('DB_USERNAME'),\n",
    "    password=os.getenv('DB_PASSWORD'),\n",
    "    host=os.getenv('DB_HOST'),\n",
    "    port=os.getenv('DB_PORT')\n",
    ")\n",
    "\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec16632-9523-42cd-b969-724f8b1b0286",
   "metadata": {},
   "source": [
    "The following SQL query retrieves the most recent submission where all subreddit data is available, representing the latest \"subreddit data fetch\" that provides the most up-to-date information about a subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4ad5e9f-6827-4666-8a5d-56517d24cec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1704059586, 'AcademicPsychology', 't5_2sluh', 'r/AcademicPsychology', 'public')\n",
      "(1704065506, 'ArtificialInteligence', 't5_3crzr', 'r/ArtificialInteligence', 'public')\n",
      "(1704057226, 'ChatGPTCoding', 't5_7ipnaj', 'r/ChatGPTCoding', 'public')\n",
      "(1704057460, 'climatechange', 't5_2rawx', 'r/climatechange', 'public')\n",
      "(1704042913, 'cogsci', 't5_2qh0k', 'r/cogsci', 'public')\n",
      "(1703999262, 'edtech', 't5_2r5zc', 'r/edtech', 'public')\n",
      "(1703980527, 'fintech', 't5_2u7f1', 'r/fintech', 'public')\n",
      "(1704043506, 'ImaginaryTechnology', 't5_2tf7t', 'r/ImaginaryTechnology', 'public')\n",
      "(1704066515, 'NLP', 't5_2qqpg', 'r/NLP', 'public')\n",
      "(1703866977, 'stocks', 't5_2qjfk', 'r/stocks', 'public')\n",
      "(1704061306, 'StocksAndTrading', 't5_2wwow', 'r/StocksAndTrading', 'public')\n",
      "(1704042061, 'stockstobuytoday', 't5_3hczbd', 'r/stockstobuytoday', 'public')\n"
     ]
    }
   ],
   "source": [
    "sql_query = '''\n",
    "\n",
    "SELECT DISTINCT ON (subreddit) created_utc, subreddit, subreddit_id, subreddit_name_prefixed, subreddit_type\n",
    "FROM submissions\n",
    "WHERE subreddit_name_prefixed IS NOT NULL \n",
    "    AND subreddit_type IS NOT NULL\n",
    "ORDER BY subreddit, created_utc DESC\n",
    "\n",
    "'''\n",
    "\n",
    "cur.execute(sql_query)\n",
    "results = cur.fetchall()\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0944ff3-cf06-473e-b2be-e276a152f88a",
   "metadata": {},
   "source": [
    "Since this query works on both `submissions` and `comments`, which have the same subreddit data attributes, we will combine their results and select the most recent entries. This will create a table called `subreddits` that contains the latest subreddit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51859739-74e2-4a23-8fab-ba2e52cda1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = '''\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS subreddits AS\n",
    "WITH submissions_comments AS (\n",
    "    SELECT created_utc, subreddit, subreddit_id, subreddit_name_prefixed, subreddit_type\n",
    "    FROM submissions\n",
    "    WHERE subreddit_name_prefixed IS NOT NULL \n",
    "      AND subreddit_type IS NOT NULL\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT created_utc, subreddit, subreddit_id, subreddit_name_prefixed, subreddit_type\n",
    "    FROM comments\n",
    "    WHERE subreddit_name_prefixed IS NOT NULL \n",
    "      AND subreddit_type IS NOT NULL\n",
    ")\n",
    "\n",
    "SELECT DISTINCT ON (subreddit) created_utc as last_updated_utc, subreddit, subreddit_id, subreddit_name_prefixed, subreddit_type\n",
    "FROM submissions_comments\n",
    "ORDER BY subreddit, created_utc DESC;\n",
    "\n",
    "'''\n",
    "\n",
    "cur.execute(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed3f3de-513b-4236-8a7f-47af4023ba2f",
   "metadata": {},
   "source": [
    "##### Query `subreddits` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e917875f-c182-4efc-b6d2-d0a9096e60f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1704062760, 'AcademicPsychology', 't5_2sluh', 'r/AcademicPsychology', 'public')\n",
      "(1704067109, 'ArtificialInteligence', 't5_3crzr', 'r/ArtificialInteligence', 'public')\n",
      "(1704065858, 'ChatGPTCoding', 't5_7ipnaj', 'r/ChatGPTCoding', 'public')\n",
      "(1704067180, 'climatechange', 't5_2rawx', 'r/climatechange', 'public')\n",
      "(1704056457, 'cogsci', 't5_2qh0k', 'r/cogsci', 'public')\n",
      "(1704061327, 'edtech', 't5_2r5zc', 'r/edtech', 'public')\n",
      "(1704022970, 'fintech', 't5_2u7f1', 'r/fintech', 'public')\n",
      "(1704055582, 'ImaginaryTechnology', 't5_2tf7t', 'r/ImaginaryTechnology', 'public')\n",
      "(1704066515, 'NLP', 't5_2qqpg', 'r/NLP', 'public')\n",
      "(1703866977, 'stocks', 't5_2qjfk', 'r/stocks', 'public')\n",
      "(1704062761, 'StocksAndTrading', 't5_2wwow', 'r/StocksAndTrading', 'public')\n",
      "(1704042061, 'stockstobuytoday', 't5_3hczbd', 'r/stockstobuytoday', 'public')\n"
     ]
    }
   ],
   "source": [
    "sql_query = '''\n",
    "SELECT * FROM subreddits;\n",
    "'''\n",
    "\n",
    "cur.execute(sql_query)\n",
    "results = cur.fetchall()\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6cf275-077f-433b-ab5d-1f6a9f4fbc44",
   "metadata": {},
   "source": [
    "#### `User`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bff11414-f567-49dd-96a4-f2663ea29f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = '''\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS users AS\n",
    "WITH submissions_comments AS (\n",
    "    SELECT author, author_created_utc, author_fullname\n",
    "    FROM submissions\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT author, author_created_utc, author_fullname\n",
    "    FROM comments\n",
    ")\n",
    "SELECT DISTINCT ON (author) \n",
    "    author, \n",
    "    COALESCE(author_created_utc, NULL) AS author_created_utc, \n",
    "    author_fullname\n",
    "FROM submissions_comments\n",
    "ORDER BY author, author_created_utc DESC;\n",
    "\n",
    "'''\n",
    "\n",
    "cur.execute(sql_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data101.venv)",
   "language": "python",
   "name": "data101.venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
